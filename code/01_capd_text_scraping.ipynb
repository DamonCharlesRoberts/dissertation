{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: CAPD Text scraping \n",
    "\n",
    "# Notes:\n",
    "    #* Description: Jupyter Notebook to scrape CAPD site for yard signs in 2018-2022 national elections\n",
    "    #* Updated: 2022-11-04\n",
    "    #* Updated by: dcr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # library to help with sleep and wait times\n",
    "from selenium import webdriver # to setup the driver\n",
    "from selenium.webdriver.chrome.options import Options # to specify options for my chrome webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager # use the driver manager so that I don't have to download it and keep track of versions myself\n",
    "from selenium.webdriver.support.ui import WebDriverWait # use this so that I can wait on my driver to load the page completely before searching\n",
    "from selenium.webdriver.common.by import By # using the By function to help with the xpath searching\n",
    "from selenium.webdriver.support import expected_conditions as EC # load the expected_conditions function to make sure all elements matching the xpath happen before the driver stops waiting on the loading\n",
    "import pandas as pd # need the pandas package for dataFrames\n",
    "import duckdb # need to store data into database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping of CAPD site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver and scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options() # set webdriver options\n",
    "options.add_argument(\"start-maximized\") # set an option for the webdriver to automatically open chrome browser that I can interact with the page\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options) # setup chrome webdriver\n",
    "\n",
    "wait = WebDriverWait(driver, 10000) # specify wait time for the page before stopping it from collecting links in xpath\n",
    "\n",
    "url = \"https://www.politicsanddesign.com/\"\n",
    "\n",
    "driver.get(url) # Make sure to turn off year filter on page that pops up and scroll all the way down\n",
    "wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@class='candidate-card-text']/ul\"))) # tell driver to search along this path, but to not stop the search until it everything has been collected\n",
    "time.sleep(10) # add an some extra time incase needed \n",
    "txt_url = driver.find_elements(By.XPATH, \"//div[@class='candidate-card-text']/ul\") # go ahead and collect all the elements\n",
    "txt_url2 = [] # make empty list object to store urls in\n",
    "for i in txt_url:\n",
    "    txt_url2.append(i.text) # grab the text for each of the sessions\n",
    "driver.close() # close the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_url4 = [i for i in txt_url2 if i] # take the elements of txt_url2 and place them in a list\n",
    "\n",
    "yard_signs = pd.DataFrame(txt_url4) # turn this list into a single column dataframe\n",
    "\n",
    "# Split the single column into 6 to document each piece of information about the candidate\n",
    "yard_signs[['Candidate_Name', 'Party']] = yard_signs[0].str.split('(', 1, expand = True)\n",
    "yard_signs[['Party', 'State']] = yard_signs['Party'].str.split('\\n', 1, expand = True)\n",
    "yard_signs[['State', 'District']] = yard_signs['State'].str.split('-', 1, expand = True)\n",
    "yard_signs[['District', 'Office', 'Year']] = yard_signs['District'].str.split(',', 2, expand = True)\n",
    "yard_signs['Party'] = yard_signs.Party.str.strip(')')\n",
    "\n",
    "# Drop that first column with the original list information \n",
    "yard_signs.drop(yard_signs.columns[[0]], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = duckdb.connect('C:/Users/damon/Dropbox/current_projects/dissertation/data/dissertation_database_db') # connect to the database\n",
    "\n",
    "database.execute(\"CREATE OR REPLACE TABLE ch_1_capd_yard_signs AS SELECT * FROM yard_signs\") # create the table\n",
    "\n",
    "database.commit() # commit this to the database\n",
    "\n",
    "database.execute(\"SELECT * FROM ch_1_capd_yard_signs\") # go to the ch_1_capd_yard_signs table in the database\n",
    "yard_signs = pd.DataFrame(database.fetch_all(), columns = ['Candidate_Name', 'District', 'Office', 'Year', 'Party', 'State']) # take that duckdb table and turn it into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- Now run 02_capd_img_scraping.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afa691b0bad08ea1edfc387adda4ed8d77b19980e4c9407f627234486b5624ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
