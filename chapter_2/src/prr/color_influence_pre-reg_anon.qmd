---
title: |
  Pre-analysis plan
subtitle: |
  How do colors convey political information and affect individual attitudes?
repo: |
  For replication, go to: <https://github.com/DamonCharlesRoberts/dissertation>.
author:
  - name: Damon C. Roberts
    email: damon.roberts-1@colorado.edu
    orcid: 0000-0002-4360-3675
    title: PhD Candidate
    affiliations:
      - id: CU
        name: University of Colorado Boulder
        department: Political Science
        address: 333 UCB
        city: Boulder
        region: CO 
        postal-code: 80309-0333
abstract: |
  Colors are important to politics as a form of political information. Building upon existing theories of political information processing, attitude formation, and affect in neuroscience, I present a snap-judgement model of political information processing. In this model, colors provide automatic information about a politically-relevant object that may shape subsequent processing of more complex information. The model has important implications for how we consider the role that visual information has on political information processing and attitude formation. The model additionally provides clarity on motivations behind party branding and the ways in which information may activate partisan biases pre-consciously. I test this model using a survey experiment tracking participants' mouse movements to view different parts of yard signs that vary the use of partisan and non-partisan colors. I additionally leverage congressional redistricting to examine changes in color use for political yard signs as a strategic choice for attracting voters by communicating partisanship.
bibliography: "../../../assets/references.bib"
format:
  hikmah-pdf:
    # Spit out in drafts directory
    latex-output-dir: "../../out/prr"
    # Use biblatex-chicago
    biblatex-chicago: true
    biblio-style: authordate
    biblatexoptions: 
      - backend=biber
      - autolang=hyphen
      - isbn=false
      - uniquename=false
code-overflow: wrap
nocite: |
  @quarto
execute:
  echo: false
  warning: true
  message: true
  cache: false
params:
  replicate: false
---

{{< pagebreak >}}

+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| Hypotheses   | Expectation                                                                                                                                         |
+==============+:===================================================================================================================================================:+
| $H_1$        | People notice color                                                                                                                                 |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| $H_2$        | Red is associated with Republicans and Blue is associated with Democrats                                                                            |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| $H_3$        | Candidates using Red are more supported by Republicans; candidates using Blue are more supported by Democrats                                       |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| $H_4$        | Republicans spend less time evaluating candidates using Red; Democrats spend less time evaluating candidates using Blue                             |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| $H_5$        | Campaigns recognize the importance of color to voters' evaluations of candidates and respond strategically                                          |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+

: Summary of hypotheses {#tbl-hypotheses}

From my review of the literature in political science, neuroscience, and communication, I expect the following: that the average potential voter pays attention to the colors used in campaign branding ($H_{1}$); that these colors that they notice shape perceptions about the person and ideological symbol represented in the branding -- meaning that they express different levels of preference for receiving more information that is similar to what they saw and their levels of preference for supporting such a campaign ($H_{2}$); that the consistency of information explains more positive perceptions -- simple visual information with more complex "traditional" information ($H_{3}$); this positive and consistent information is processed quicker than negative and inconsistent, negative and consistent, and positive and inconsistent information ($H_{4}$); and finally that campaigns make strategic choices about their branding to attract voters ($H_{5}$) in line with their primary objective of reelection [@fenno_1973_lb; @mayhew_1974_yup]. A summary of these hypotheses are included in @tbl-hypotheses.

# Pre-test {#sec-pre-test}

```{r}
#| label: setup-pre-test-block
# set working directory
#setwd("./chapter_2/src/prr")
# Set seed
set.seed(12102022)
#setwd("./chapter_2/src/prr")
# Source cleaning script
source("eda/pre-test/01_pre-test_cleaning.R")
# Load relevant functions
if (params$replicate == TRUE){
  box::use(
    modelsummary[
      datasummary_skim
    ]
    ,marginaleffects[
      avg_slopes
      ,posterior_draws
      ,plot_slopes
    ]
    ,ggplot2[
      ggplot
      ,aes
      ,theme_minimal
      ,labs
      ,scale_y_discrete
    ]
    ,ggdist[
      stat_halfeye
    ]
    ,brms[
      brmsformula
      ,cumulative
      ,prior
    ]
    ,rstan[stan_model]
    ,./R/sim_all[sim_all]
    ,./R/discrepancy_df[discrepancy_df]
    ,./R/true_positive_hist[true_positive_hist]
  )
} else {
  box::use(
    modelsummary[
      datasummary_skim
    ]
    ,marginaleffects[
      avg_slopes
      ,posterior_draws
      ,plot_slopes
    ]
    ,ggplot2[
      ggplot
      ,aes
      ,theme_minimal
      ,labs
      ,scale_y_discrete
    ]
    ,ggdist[
      stat_halfeye
    ]
    ,./R/discrepancy_df[discrepancy_df]
    ,./R/true_positive_hist[true_positive_hist]
  )
}
    #* default tails
options("marginaleffects_posterior_interval" = "hdi")

```

I conducted a pre-test in November 2019 with a sample of over 400 undergraduate students at a large university in the northwestern region of the United States. I recruited students enrolled in a political science course and were offered extra credit for their participation in the study. The study asked participants to participate in 5 survey experiments administered by those affiliated with the university's college-level unit. These other survey experiments focus on capturing local policy issues around urban design and criminal justice and probing participants about political participation in local and national-level elections. Subjects participated in my survey experiment after one that examined their levels of political participation in local, state, and national elections.

```{r}
#| label: tbl-pre-test-descriptive-stats
#| tbl-cap: Descriptive statistics of pre-test measures

data[["clean"]][ # select the following columns
    ,list(Female, White, age, PartyId, BlueTreatment, RedTreatment, Vote)
] |>
datasummary_skim( # make a table of descriptive statistics
    notes = c( # add some notes to the table
        "Data source: Pre-test experiment.",
        '"Unique" column includes missing values.'
    )
)
```

Using the `modelsummary` [@modelsummary] package implemented in the `R` language [@r_software], I create @tbl-pre-test-descriptive-stats which presents the descriptive characteristics of the sample. The sample is primarily White with over 80% self-reporting that they are White(coded as: 0 = non-White, 1 = White). The sample also skews slightly female on sex, with about 60% reporting that they are female (coded as 0 = Male, 1 = Female). The sample also, unsurprisingly, skews young, with the average respondent reporting an age of about 22 years old. The average respondent also appears to be an independent but leans Republican (coded as -3 = strong Democrat, -2 = Democrat, -1 = leans Democrat, 0 = Independent, 1 = leans Republican, 2 = Republican, 3 = strong Republican).

I randomly assign participants to three conditions. The conditions prompt subjects to "Imagine that \[they\] are driving along a road and see this yard sign" with the same message "Vote for Riley." The conditions vary on the color of the background for the image.[^color_influence_pre-reg_anon-1] In the control condition, the background was white. Then I had a red yard sign and a blue yard sign condition. I asked participants questions operationalized as outcomes of interest on a separate screen.

[^color_influence_pre-reg_anon-1]: The Appendix contains the images used for the treatments and the particular wording for the dependent variables.

To provide a preliminary test of $H_1$ and $H_2$, I ask participants to report whether the candidate was a "Republican, Democrat, or Independent." I test whether participants presumed that the candidate was of a particular partisan persuasion based only on the color choice of the yard sign alone. I created two indicator variables of the treatment the subjects received: whether or not they had the blue or red yard sign treatment. The placebo condition (the white yard sign treatment) is treated as the baseline condition when including both treatment indicator variables in the model.

To examine whether subjects presumed a partisan affiliation of the fictional candidate, I use the `brms` [@brms] implementation of `STAN` to fit an ordinal logistic regression model using the cumulative PDF with a inverse logit link function. My model specification is reflected in @eq-party-specification. I use posterior prediction checks after fitting the model to examine whether the predicted values from the posterior draws resemble my observed values. I calculate the average marginal effects across my posterior draws with High Density Intervals (as opposed to Equal Tail Intervals) and plot the distributions of these. In @fig-party-pre-test-ames, the bars reflect the 90% and 95% credible intervals whereas the dot reflects the median posterior draw.

$$
\begin{split}
  \alpha \sim Student-T(3,0,2.5) \\
  \beta_i \sim Normal(0, 1) \\
  \hat{Party_i} = Cumulative(logit^{-1}(\alpha_1 + \alpha_2 + \beta_1 \times Red Treatment + \beta_2 \times Blue Treatment))
\end{split}
$$ {#eq-party-specification}

```{r}
#| label: party-pre-test-model
#| include: false

if (params$replicate == TRUE) {
  PartyFormula <- bf(
    Party ~ RedTreatment + BlueTreatment
    ,family = cumulative(link = "logit")
  )
  PartyModel <- brm(
    formula = PartyFormula,
    ,data = data[["clean"]]
    ,chains = 1
    ,silent = 0
  )
  # Store model object
  save(
    PartyModel
    ,file="../../data/models/prr/pre-test/pre-test_party_model.RDS"
  )
} else (
  # load the stored model object
  load(
    file="../../data/models/prr/pre-test/pre-test_party_model.RDS"
  )
)

```

```{r}
#| label: fig-party-pre-test-ames
#| fig-cap: Effect of treatments on party perceptions

partyAMES <- avg_slopes(
  PartyModel,
  type = "link"
) |>
posterior_draws()

ggplot(
  data = partyAMES,
  aes(
    x = draw,
    y = term
  )
) +
stat_halfeye(
  slab_alpha = 0.45
) +
theme_minimal() +
labs(
  x = "Average Marginal Effect",
  y = "",
  caption = "Data source: Pre-test.\n Distribution of AME's for model posterior draws using HDI's.\n Interpreted as the logged-odds of an outcome occurring when holding the other independent variables at their mean value.\n Dot reflects median posterior draw where bars reflect 90% and 95% credible intervals."
) +
scale_y_discrete(
  labels = c(
    "Blue Treatment",
    "Red Treatment"
  )
)
```

The results suggest that for those in the red yard sign treatment condition perceive the candidate to be a Republican. Whereas, those assigned to the blue yard sign treatment condition perceived the owner of the yard sign to be a Democrat. This is without *any* information about the political candidate. Both of these treatments have a quite large effect on conveying the partisanship of the candidate. This finding fits with the expectation that individuals have internalized the use of color in party branding and are independently capable and willing to make that connection. This preliminary evidence suggests that color likely conveys important information to the public.

Individuals associate Republicans with the color Red and Democrats with the color Blue. Do these associations influence their attitudes? In addition to asking participants about whether they perceived the candidate was Republican or Democratic, prior to that question, I asked participants whether they would be willing to vote for the candidate and whether they would want to avoid engaging with the candidate's campaign materials in the future. I used these two questions to construct a measure of willingness to engage with the fictional candidate. Those who reported that they do not want to vote for the candidate *and* want to avoid the candidate in the future I coded with a value of 1. Those who reported a mix of responses were coded as 2. Finally, those that reported that they were willing to vote for the candidate *and* wanted to receive more information about the candidate were coded as 3. Again, the *only* information they have about the candidate is conveyed on the yard sign. I fit an ordinal logistic regression model again using the `brms` [@brms] implementation of `STAN`. My model specification is conveyed in @eq-vote-specification.

$$
\begin{split}
  \alpha \sim Student-T(3,0,2.5) \\
  \beta_i \sim Normal(0, 1) \\
  \hat{Party_i} = Cumulative(logit^{-1}(\alpha_1 + \alpha_2 + \beta_1 \times Red Treatment + \beta_2 \times Blue Treatment \\
    + \beta_3 \times Party Identification (3-item) \\
    + \beta_4 \times Red Treatment \times Party Identification (3-item) \\
    + \beta_5 \times Blue Treatment \times Party Identification (3-item)))
\end{split}
$$ {#eq-vote-specification}

I expect that the effect of the treatment on willingness to engage more with the candidate, either in terms of learning more [@lodge_taber_2013_cup] and voting [@broockman_kalla_2022_ajps] is dependent on whether that candidate is a co-partisan. I also examined this model with a variety of priors and settled on priors I believe to reflect maximum entropy principles [@mcelreath_2020_crc] and those that produced posterior predictive checks with predicted values from the model resembling those observed in my data. I use the `marginaleffects` package [@marginaleffects] to calculate and plot the conditional marginal effects.

```{r}
#| label: pre-test-vote-model
#| include: false
if (params$replicate == TRUE) {
  # fit the model
  VotePreTestFormula <- bf(
    Vote ~ RedTreatment + BlueTreatment + PartyId + RedTreatment * PartyId + BlueTreatment * PartyId
    ,family = cumulative(link = "logit")
  )
  VotePreTestModel <- brm(
    formula = VotePreTestFormula
    ,data = data[["clean"]]
    ,chains = 1
    ,silent = 0
  )
  # Store model object
  save(
    VotePreTestModel
    ,file="../../data/models/prr/pre-test/pre-test_vote_model.RDS"
  )
} else {
  # load the stored model object
  load(
    file="../../data/models/prr/pre-test/pre-test_vote_model.RDS"
  )
}
```

```{r}
#| label: fig-pre-test-vote-cmes
#| layout-nrow: 2
#| fig-cap: "Effect of yard sign color on candidate evaluation"
#| fig-subcap:
#|  - "Republicans would vote for candidate with Red yard sign"
#|  - "No difference among partisans in support for candidate with Blue yard sign"
#| fig-width: 6
red_cme <- plot_slopes(
    VotePreTestModel,
    variables = "RedTreatment",
    condition = "PartyId",
    type = "link"
) +
theme_minimal() +
labs(
    x = "Party Identification",
    y = "CME of Red Treatment",
    caption = "Data source: Pre-test.\n Conditional Marginal Effects of Red Treatment upon support for candidate.\n Interpreted as the logged-odds of voting for fictional candidate\n conditional on varying levels of Party Identification.\n Uncertainty reflected by predicted posterior draws at the 95% level with HDI's."
)

blue_cme <- plot_slopes(
    VotePreTestModel,
    variables = "BlueTreatment",
    condition = "PartyId",
    type = "link"
) +
theme_minimal() +
labs(
    x = "Party Identification",
    y = "CME of Blue Treatment",
    caption = "Data source: Pre-test.\n Conditional Marginal Effects of Blue Treatment upon support for candidate.\n Interpreted as the logged-odds of voting for fictional candidate\n conditional on varying levels of Party Identification.\n Uncertainty reflected by predicted posterior draws at the 95% level with HDI's."
)

red_cme
blue_cme
```

@fig-pre-test-vote-cmes presents results suggesting that among Republicans receiving the red treatment, they are more likely to indicate a positive valence toward the candidate than Democrats, who are more likely to report a negative evaluation of the candidate. While Democrats receiving the blue treatment are more likely to report a positive valence toward the candidate relative to Republicans, the effect is plausibly zero. This may be an artifact of asymmetric political polarization. Some scholars suggest that Republicans are much more group-oriented than Democrats [see @lupton_et-al_2020_bjps]; these results may fit with such a narrative. Republicans are reactive to those they may presume to be co-partisan in a way that Democrats do not appear to be as reactive in a similar magnitude.

The evidence from this pre-test is limited. The experimental design is not testing pre-conscious evaluations. The treatments are explicitly political and rely upon a convenience sample of those enrolled in political science classes. The ability of individuals to associate partisanship with the treatments is likely overstated. The sample is also quite unrepresentative, so the inference is indeed threatened. As I discussed with the third model, I also have omitted variable bias as the result of my outcome is a better measure of approach behaviors for those who are extroverted.

Though there are problems with the design here, it does serve some purpose for this project. It demonstrates that despite its problems, color can betray information at some level when thinking of politics. It also provides a functional first test of a possible research design to examine what this particular design can and cannot tell me about my proposed mechanism.

# Study 1 {#sec-study-1}

The purpose of [Study 1](#sec-study-1) is to examine the average treatment effect [^color_influence_pre-reg_anon-2] of color in a common form of campaign branding -- yard signs -- on the attitudes of the observer. Evidence suggests that yard signs *do* matter in shaping political attitudes [@makse_et-al_2019_oup], vote intentions [@makse_et-al_2019_oup], and even electoral outcomes [@green_et-al_2016_es]. As they are simple, cheap, intrusive, and common forms of campaign branding, they provide a conservative test of the effects of color in campaign branding. Using yard signs in this study, I test the first four hypotheses I derive from the snap-judgment model. First, [Study 1](#sec-study-1) tests the claim that individuals do indeed notice the color of electoral yard signs. Next, it tests the claim that these colors that individuals detect influence the viewer's evaluations of the yard sign and the candidate represented on it. [Study 1](#sec-study-1) then tests the claim that the effects on perception are moderated by how consistent the color is with the more complex information displayed on the yard sign. Furthermore, finally, [Study 1](#sec-study-1) examines whether positive information that contains consistency is processed quicker than negative or inconsistent information. That is, do partisans quickly detect and encode information from a clearly co-partisan political candidate?

[^color_influence_pre-reg_anon-2]: $\frac{1}{n}\sum{}{i}(Y_i(d=1) - Y_i(d=0))$ [see @lundberg_et-al_2021_asr]

## Design

I recruit participants from Prolific.[^color_influence_pre-reg_anon-3] After providing informed consent to participate in the study, Prolific redirects subjects to Pavlovia[^color_influence_pre-reg_anon-4] and are provided with a demographics and political attitudes questionnaire. As I am concerned about priming effects introduced by these questions, while I am simultaneously concerned about bias introduced by post-treatment control [@montgomery_et-al_2018_ajps], I randomly select half of the participants to receive the questionnaire post-treatment and the other half receive it pre-treatment. This questionnaire includes common questions about the participant's ascriptive and descriptive characteristics and about the participant's political ideology, partisan identification, interest in politics, patriotism, and political knowledge.

[^color_influence_pre-reg_anon-3]: I pay subjects a rate of \$12.00 per hour. On top of the price per participant, Prolific charges a 30% servicing fee.

[^color_influence_pre-reg_anon-4]: Pavolovia allows for researchers to host and run open source experiments for about \$0.20 per participant (to cover their server costs). I use it primarily to integrate the JavaScript components from the jsPsych package [@jspsych] for my experimental design.

I include those questions in the questionnaire due to expectations that they may act as confounds in my hypotheses. As political knowledge intertwines with the strength to which an individual identifies with a political party [@delli-carpini_keeter_1996_yup], I expect that political knowledge is an important confounder in my tests of $H_2$, $H_3$, and $H_4$. Therefore, I include a standard battery (the American National Election Study's battery) for assessing political knowledge.[^color_influence_pre-reg_anon-5] As political knowledge is shaped by levels of interest in politics as well [@delli-carpini_keeter_1996_yup], I include the American National Election Study's question to assess levels of self-reported interest in politics. I include a battery to measure patriotism. For this measure, participants can respond to values between 1 (not proud at all) and 4 (very proud) to the following questions: "When I hear the American national anthem, it makes me feel" and "when I say the American pledge of allegiance, it makes me feel" [@huddy_khatib_2007_ajps; @perez_et-al_2019_jop]. I additionally include some questions collecting information on participants' ascriptive and descriptive characteristics such as age, education, gender identity, and racial identity, as a number of these are correlates with partisan identification [see @campbell_et-al_1960_jws; @mason_2018_cup].

[^color_influence_pre-reg_anon-5]: A table including the wording of this, and all, measures are included in the Supplementary Information.

Additionally, I include a question about the respondent's sex assigned at birth and about whether they have received a diagnosis of any color blindness. As some individuals may possess undiagnosed colorblindness, asking about their sex assists in covariate balance. I additionally include an open-ended question asking participants to describe their "first memory of a political event." The use of an open-ended question helps provide an attention check and identify duplicated responses for those spoofing IP addresses with a VPN [see @kennedy_et-al_2021_poq]. Given estimates using these exclusion criteria, I would expect that upwards of 40% of my original sample will fail these attention checks [@kennedy_et-al_2021_poq]; thus, the large sample size.

I then present participants with an instruction screen informing them of the task for the experiment. In the first trial of the experiment, participants I randomly presented participants with two of three possible yard signs, one at a time. These yard signs are simple with the text "Riley Ready to Lead" and a solid background color of either "Republican Red," "Democratic Blue," or White.[^color_influence_pre-reg_anon-6] There is an added component to this, however.

[^color_influence_pre-reg_anon-6]: See the supplementary information to view all of the stimuli used in [Study 1](#sec-study-1).

Rather than use eye-tracking devices and software, I instead use Mouseview.js [see @anwyl-irvine_et-al_2022_brm], which either blocks out or blurs a large portion of the participant's screen and encourages them to move their mouse to view different parts of the screen in isolation. As the participants move their cursor around the screen, it tracks the coordinates of the cursor along with the "dwell" time of the cursor in that particular coordinate. One primary benefit of Mouseview.js is that it allows researchers to field their experiments outside of a lab-based setting -- while providing results that robustly correlate with the results from a design employing eye-tracking hardware [@anwyl-irvine_et-al_2022_brm]. This allows researchers to rely less on convenience samples, which are common with eye-tracking studies. For my design, I am particularly concerned about reliance on a convenience sample due to variations in participants' ability to detect and process color in the U.S. population relative to a student sample. This means that Mouseview.js is a handy tool for my study. A published pilot version of the experiment used in [Study 1](#sec-study-1) on [Pavlovia](https://run.pavlovia.org/damoncroberts/diss_ch_1_pre-reg/?__pilotToken=c74d97b01eae257e44aa9d5bade97baf&__oauthToken=9a645f267f9b83c8ebb48e60093bae5a746da56a6c824b8343389a9329fa2365).

When viewing each yard sign in the first trial, there is a blur over a substantial portion of the screen. At any given point in time, participants can view only 8% of the image without an obstruction, which simulates the observation that we typically foveate on about 8% of our available visual field at any given time [@wedel_pieters_2008_rmr].[^color_influence_pre-reg_anon-7] Participants move their cursor to explore the yard sign. I allot 5000 ms to perform the exploration until the image goes away to encourage a consistent and short duration to explore the image.[^color_influence_pre-reg_anon-8] After exploring each image, I asked participants what colors were on the yard sign and whether they felt that the candidate represented on the yard sign was a Democrat, a Republican, or Neither. After viewing both images, I ask subjects to indicate their preference among the two images. To ensure that participants have a standardized initial placement of their cursor, I display a blank page before viewing the yard sign that requires participants to click a "Next" button. Immediately after clicking "Next," participants are shown the yard sign. The goal is to ensure that variation in where participants explore the image is not dependent on a non-standard starting point for their cursor. I additionally utilize a gaussian blur for the overlay of the image rather than a solid overlay obstructing the participant's view. This gaussian blur allows participants to see a blurred visual field beyond the cursor. This allows participants to see enough to take purposeful action to explore blurred parts of the image that attract them [@anwyl-irvine_et-al_2022_brm]. The use of the gaussian blur requires that participants use a web browser other than Safari because of a known issue [@anwyl-irvine_et-al_2022_brm]. This will require participants to either switch browsers or to not participate in the study if they are using Safari at the time they are recruited by Prolific to participate in the study. As this requirement is enforced *before* joining the study, this should not have an effect on the number of excluded participants from my original sample nor on a nationally representative original sample.

[^color_influence_pre-reg_anon-7]: I include a screenshot providing an example of what the participants are able to see with the blur included in the supplementary materials.

[^color_influence_pre-reg_anon-8]: In marketing research, some studies give participants about 6000 milliseconds in eye-tracking studies to examine a brand and to formulate an intention to purchase a product or not [@wedel_pieters_2008_rmr]. With Mouseview.js, a study examining the tool's correlation with optical responses to viewing disgust-and-pleasure-evoking images uses 10000 milliseconds; but is intended to be an extended amount of time [@anwyl-irvine_et-al_2022_brm].

There are three more trials that are much like the first trial. What is different between the two other trials is that I vary the amount of color that is on the yard signs (trials 2 and 3). I provide more textual information that deviates from the association of Republicans with red and Democrats with blue (trial 4). Examples of all of these yard signs are included in the supplementary materials.

## Do individuals notice color in political branding?

To address this first question, I use two measures of a participant's attention toward the colors on the yard sign. I collected the first measure through a question posed to the participant after viewing each yard sign, "what color was the yard sign?". The second measure is more implicit than the first: it accounts for the time someone's mouse hovered over the non-text elements of the yard sign relative to how long their mouse hovered over the text. The self-reported measure allows us to examine the conscious detection of color, while the more implicit measure allows us to examine where individuals' attention goes: toward the color or the text.

This is primarily a descriptive exercise, and I intend to compare the differences in measures between the "non-partisan" (white) yard signs and the partisan (red and blue) yard signs. I will examine the differences in measures between the partisan yard signs among self-identified partisan respondents. I additionally include additional models that interact partisanship with age and political knowledge. As the consistent usage by the parties to use the colors red and blue did not occur until the 2000 presidential election, those that were in their early adult years during that time experienced partisan politics where colors were not a strong cue. I additionally expect that those who are more politically knowledgeable should rely on these partisan cues more as they tend to be stronger partisans [@delli-carpini_keeter_1996_yup].

## Do colors shape perceptions of political objects?

To address the following question of whether the colors affect perceptions of the candidate and the yard sign, I ask participants to report whether they perceived the candidate to be a partisan -- either Republican, Democrat, or as non-partisan immediately after viewing each image. Everything on the yard signs remains constant except for the color. As representations of ideology are associated with more than just political views but things like space [@mills_et-al_2016_bbr] and color [@maestre_medero_2022_pr], differences between respondents on the perceived political affiliations of the candidate should be more than stochastic differences. However, differences occur based on the associations between red and blue with partisanship and the lack of political information that the color white conveys.

I examine differences in respondents' reported perceptions of the candidate's partisan affiliation. As respondents are making the choice between two yard signs, my estimand of interest is of a pairwise comparison of the two yard signs they see in each trial. While these are most common in ranking athletic teams, there is recent academic interest in these estimands as they allow researchers to examine the "popularity" of a particular treatment when experimental participants are asked to choose between two options [see @hopkins_noel_2022_ajps]. As color is the only information that changes between yard signs in a given trial, pairwise comparisons are a more appropriate estimand than those from conjoint analyses which often produce treatments that simultaneously vary a number of dimensions of a profile [@bansak_et-al_2021_cup]. There are a handful of options about how to calculate a pairwise comparison, but I elect to estimate a Bradley-Terry score. Bradley-Terry scores are designed for determining "winners" that account for a variety of predictors [@schnakenberg_penn_2013_pa]. Unlike Elo Scores, a popular alternative, Bradley-Terry scores do not require every pairwise matchup to determine a winner.

## Do these perceptions require consistency between information types?

Another hypothesis derived from the snap-judgment model suggests that inconsistency in the yard sign's visual information will lead to more mixed perceptions of the candidate's political stances. The design of the latter trials in the study presents yard signs with mixtures of non-partisan and out-partisan colors. For example, primarily red and presumably Republican yard signs, but has some blue or white in them.

I take the same analytical strategy to address this question as I did before. I estimate the Bradley-Terry scores among respondents. I should expect that the trials that use less "consistent" visual information demonstrate more ambivalence among respondents in their reported perceptions of the politician's positions. Specifically, I expect that participants will perceive the yard signs that have both red and blue on them as more moderate and that the higher proportion of the color red or blue among the two colors will lead respondents, on average, to be more likely to believe that the candidate leans more Republican or Democratic.

## Do partisans process co-partisan branding faster?

The other analytical tasks do not examine expectations derived from the motivated reasoning portion of the model. That is, do people process in-group information faster than out-group information? There is evidence of these tendencies in political circumstances [@lodge_taber_2013_cup].

To examine whether motivated reasoning is also active with the processing of politically-relevant color, I examine the difference between the amount of time between the start of viewing a stimulus and clicking "Next" to stop viewing the stimulus among those who were viewing a presumed co-partisan yard sign relative to those viewing a presumed out-partisan yard sign. As motivated reasoning tends to be more prevalent among strong partisans and those highly knowledgeable and interested in politics [@lodge_taber_2013_cup], I control for responses to the pre-treatment knowledge, interest, and partisan strength batteries.

## Checking the robustness of my analytical strategy with simulations

The [Pre-test](#pre-test-sec-pre-test) provides a lot of useful information that I might use to construct my expectations for what I'll discover in [Study 1](#study-1-sec-study-1). Rather than evaluating my design choices based on expected long-run tendencies, I explicitly use the results from the [Pre-test](#pre-test-sec-pre-test) and adjust that information with assumptions about differences between the [Pre-test](#pre-test-sec-pre-test) and [Study 1's](#study-1-sec-study-1) design to construct expectations that directly feed into my analytical choices. That is, rather than evaluating my design in the Frequentist framework, I rely on results from the [Pre-test](#pre-test-sec-pre-test) to help with constructing my priors that I will use in a Bayesian model specification. Bayesian models often face criticism for the seemingly subjectivity introduced into a model specification by including one's priors into the calculation of one's estimates. While there has been a lot of ink spilt debating the validity of this claim, a Bayesian model specification seems particularly useful in the context of pre-registering a study. The [Pre-test](#pre-test-sec-pre-test) provides results which give me a benchmark by which I might refine my expectations about the effect of color on people's perceptions about politics; it gives me a concrete $\beta$ coefficient. I can then use this information to construct my priors for my model. As I admit to a number of problems with my [Pre-test](#pre-test-sec-pre-test), my priors will reflect my uncertainty about whether those particular estimates should hold in [Study 1](#study-1-sec-study-1) and allows me to define an interval of effect sizes as opposed to a point estimate. These priors are updated by my likelihood function that is constructed by the data I collect in [Study 1](#study-1-sec-study-1). What this implies is that the more data I have, even if my priors are incorrect, I will converge upon the effect in the population [@johnson_et-al_2021_crc]. However, since my priors require uncertainty, the weight of my priors on my eventual estimate are weaker the more uncertainty I specify. The lessons this provides for a pre-registered report suggests that I can use the results from my [Pre-test](#pre-test-sec-pre-test) to inform choices for [Study 1](#study-1-sec-study-1), but that, though I use that information to inform my choices for the pre-registered report, I explicitly define how I expect that the results for [Study 1](#study-1-sec-study-1) are different than those in [Pre-test](#pre-test-sec-pre-test) and that the data for [Study 1](#study-1-sec-study-1) directly update my expectations set by [Pre-test](#pre-test-sec-pre-test).

As my estimation strategy does not assume long-run convergence to the population, many assume that my Bayesian models are agnostic to the sample size and often eschew power analyses. However, as estimates from a Bayesian model are weighted by the observed data and my priors, I want to examine the robustness of my model specification using simulated data that vary on sample size. While I do expect that my priors will shift slightly depending on my pre-registered analyses here and upon exploratory data analysis once I collect my data, using simulated data to examine my analytical pipeline offers the opportunity to demonstrate my model specification's ability to converge upon parameters defined by a hypothetical data generating process.

Using the `fabricatr` [@fabricatr] package, I simulate a population with an N of 1,00,000. The specified data generating processes for each of the variables are included in @lst-sim-dgp and @lst-sim-dgp-2. I then generate 500 random samples for each sample size of 200, 400, 600, and 800 participants. This sample size is not the size of the total sample I intend to recruit but is the total sample that I have data on after excluding those who fail attention checks and those who provide duplicate responses. Some estimate that insincere responses account for about 40% of a researcher's original sample [@kennedy_et-al_2021_poq]. However, it is important to note that this is a quite conservative estimate of the proportion of insincere responses as this estimate comes from platforms that are notorious for poor convenience samples such as MTURK.

``` {#lst-sim-dgp .r lst-cap="Code to generate simulated population data"}

dgp <- fabricate(
    N = 100000, # N in the population
    E = rnorm(N), # epsilon term
    age = round( # define age variable
        runif(N, 18, 85)
    ),
    gender = draw_binary( # define binary gender identity variable
        N,
        prob = 0.5
    ),
    white = draw_binary( # define white indicator variable
        N,
        prob = 0.6
    ),
    PartyId = draw_ordered( # define party identity variable as 3-item ordinal
        x = rnorm(
            N,
            mean = 0.4 * age - 0.6 * gender + 0.7 * white + E
        ),
        breaks = c(
            -Inf, 20.14, 23.01, Inf
        )
    ),
    Attention = draw_ordered( # define Attention variable as 5-item ordinal
        x = rnorm(
            N,
            mean = 0.5 * age - 0.3 * gender + 0.1 * white + E
        ),
        breaks = c(
            -Inf, 16.5, 28.26, 36.54, 43.82, Inf
        )
    ),
    Knowledge = rnorm( # define Knowledge variable
        N,
        mean = 0.6 * age - 0.5 * gender + 0.2 * white + 0.8 * Attention + E
    )/100,
    ...
```

``` {#lst-sim-dgp-2 .r lst-cap="Code to generate simulated population data (continued)"}
  ...
    RedTreatment = draw_binary( # Simulate treatment assignment with prob 1/3
        N,
        prob = 1/3
    ),
    BlueTreatment = draw_binary( # Simulate treatment assignment with prob 1/3
        N,
        prob = 1/3
    ),
    PartyGuess = draw_ordered( # Define PartyGuess outcome variable as 3-item ordinal
        x = rnorm(
            N,
            mean = (
              2 * RedTreatment + -2 * BlueTreatment - 0.01 * age 
              - 0.1 * RedTreatment * age + 0.1 * BlueTreatment * age 
              + 0.1 * Attention + 0.1 * Knowledge + E
            )
        ),
        breaks = c(
            -Inf, -0.5, 0, Inf
        )
    ),
    PartyGuessTrialTwo = draw_ordered( # Define PartyGuess outcome variable as 3-item categorical
        x = rnorm(
            N,
            mean = (
              2 * RedTreatment + -2 * BlueTreatment - 0.01 * age 
              - 0.1 * RedTreatment * age + 0.1 * BlueTreatment * age 
              + 0.1 * Attention + 0.1 * Knowledge + E
            )
        ),
        breaks = c(
            -Inf, -0.5, 0, Inf
        )
    ),
    Vote = draw_ordered( # Define Vote outcome variable as indicator variable
        x = rnorm(
            N,
            mean = (
              1 * RedTreatment + - 1 * BlueTreatment - 0.01 * age 
              + 0.1 * Attention + 0.1 * Knowledge + E
            )
        ),
        breaks = c(
            -Inf, 0, Inf
        )
    )
)
```

To examine my analytic strategy's ability to converge upon the population's data generating process for respondents' perceptions of the fictional candidate's partisanship, I specify a ordinal logistic regression where I set relatively constrained priors upon the beta coefficients, as depicted in @eq-party-guess-priors.

$$
\begin{split}
  \alpha \sim Student-T(3,0,2.5) \\
  \beta_i \sim Normal(0, 1) \\
  \hat{Party_i} = Cumulative(logit^{-1}(\alpha_1 + \alpha_2 \\ 
   + \beta_1 \times Red Treatment + \beta_2 \times Blue Treatment \\
   + \beta_3 \times age + \beta_4 \times Red treatment \times age + \beta_5 \times Blue treatment \times age \\
   + \beta_7 \times Attention + \beta_7 \times Knowledge))
\end{split}
$$ {#eq-party-guess-priors}

These priors suggest that I believe 68% of my beta coefficients on a logged-odds scale, given the data, should be between -1 and 1; with the median of the estimates at 0. This is quite a conservative estimate given the results from my pre-test. This is also a quite conservative estimate given that the primary %\beta$ coefficients of interest are both at 0.1 on the logged-odds scale.

After fitting each model, I construct 95% high density interval credible intervals from the model's posterior draws. I then record a value of 1 if the credible interval does not contain zero (true positive) and a value of 0 if the credible interval does (false negative) for each parameter. Once I have run each of my models for the specified sample size, I calculate the average of true positive and false negatives. This gives me a percentage of the time that I would come to the correct conclusion that there is a non-zero effect on that parameter. @fig-true-positive-rate documents these calculations for each sample size.

```{r}
#| label: load-simulation-results

if (params$replicate == TRUE) {
  # Define model arguments
  formula <- brmsformula(
    PartyGuess ~ RedTreatment + BlueTreatment + age +
    RedTreatment:age + BlueTreatment:age +
    Attention + Knowledge
  )
  compiled <- stan_model(
    "./STAN/vote_guess_simulated_model.stan"
    ,model_name="partyGuess"
  )
  family <- cumulative(link="logit")
  priors <- prior(
    stats::Normal(0,1)
    ,class=b
  )
    # Run model
  modelDiscrepancies <- sim_all(
    n=c(200,400,600,800)
    ,num_samples=500
    ,formula=formula
    ,compiled=compiled
    ,family=family
    ,priors=priors
    ,model="partyGuess"
  )
    # store model
  save(
      modelDiscrepancies
      ,file="../../data/models/prr/sim/sim.RDS"
  )
} else {
  load(
    file="../../data/models/prr/sim/sim.RDS"
  )
}
```

```{r}
#| label: fig-true-positive-rate
#| layout-nrow: 4
#| fig-width: 6
#| fig-cap: True positive rate
#| fig-subcap: 
#|  - "Red treatment"
#|  - "Blue treatment"
#|  - "Age"
#|  - "Red treatment x Age"
#|  - "Blue treatment x Age"
#|  - "Attention"
#|  - "Knowledge"
cleanNames <- c(
  "Sample size"
  ,"Red treatment"
  ,"Blue treatment"
  ,"Age"
  ,"Red treatment x Age"
  ,"Blue treatment x Age"
  ,"Attention"
  ,"Knowledge"
)
truePositiveDF <- discrepancy_df(
  list=modelDiscrepancies
  ,new_names=cleanNames
)
plotList <- lapply(
  cleanNames
  ,true_positive_hist
  ,df=truePositiveDF
)
plotList[[2]]
plotList[[3]]
plotList[[4]]
plotList[[5]]
plotList[[6]]
plotList[[7]]
plotList[[8]]
```

The results of my simulation suggest that I should aim for a sample size greater than 400 once I've excluded insincere responses. Given that my estimates of insincere responses and of effect size are quite conservative, I will aim for a total sample size of 1000 participants for Study 1.

# Study 2 {#sec-study-2}

[Study 1](#sec-study-1) presents evidence in support of the snap-judgment model in a relatively artificial way. The yard signs presented as stimuli are elementary in information content and design. Furthermore, viewing yard signs in real-world scenarios does not occur through a computer screen with an overlay allowing individuals to observe only a fraction of the screen simultaneously.

One way to examine the real-world effects of color on decisions about campaign branding is to go directly to the experts. If the effects of [Study 1](#sec-study-1) hold up in natural conditions, we should expect that campaigns make strategic decisions to nurture this behavior. Descriptive evidence suggests that female candidates in the United States make different branding decisions based on the constituency and whether their opponent is a man or a woman [@williams_et-al_2022_jomp]. If the color of a yard sign does indeed matter and its electoral effects depend on the perceptions of potential voters, we should expect that campaigns are less likely to place yard signs with a large amount of Red or Blue in more moderate districts. However, in districts where there is a strong partisan base, we should expect that the candidate of the popular party will use the party's color more, while the disadvantaged candidate will use their party's color less as a way to either play up or down their loyalty to their party.

## Are colors on yard signs a strategic choice by campaigns?

In this first part of [Study 2](#sec-study-2), I do a number of informal interviews with managers of national, state, and local campaigns. In these interviews, I simply ask the campaign managers: "what goes into your decision for the colors to put on your yard sign?" The goal of this open-ended question is to hear from the practitioners whether they perceive the yard sign color as a strategic decision for the campaign. I then follow-up to ask "do you think that the color choices you make for the yard signs matter to voters?" The goal of this question is to examine whether practitioners intuitively perceive their strategic choices to have efficacy in the election. I then ask them "if colors matter and you choose to use them strategically, do you use certain colors to attract certain groups of voters that may not be reliably in your base of support?" The goal of this question is to get at the argument that the colors that campaigns use are meant as a way to communicate the partisan and ideological alignment of the candidate with the voters. Finally, I get directly to the point "what information about the candidate are you trying to convey to potential voters when you choose the colors on your yard signs?"

While it is useful to hear from practitioners whether they strategically use colors for their yard signs and whether they believe that their strategic choices matter, there are many open questions. Firstly, the selection of whom to interview is dependent on convenience and the accessability of the campaign manager. In other words, some campaign managers may be quite hard to contact due to sparse information about them or they may be hard to contact as a result of their schedules. Second, it is doubtful that campaign managers will openly admit that the choices they make are not strategic or that they think their strategic choices have little effects on the campaign's outcome.

To address these remaining questions, I employ a second part of [Study 2](#sec-study-2) which relies upon observational data and leverages congressional redistricting following the 2020 National Census to examine whether the partisan context of a district is associated with changes to color choices for campaigns in that same district. As this is likely a very endogenous process -- where voters are influenced by the colors used and the voters influence the colors that are used -- the primary goal of this second part of the study is to demonstrate that the parties are *responding* to their intuition that suggests that colors matter to the campaign; it is not to make any directional claim that partisan composition of a district *determines* the colors on a yard sign.

## Do branding choices reflect electoral context?

```{python}
#| label: some-script-execution
#| eval: false
import os
os.system('../study_2/code/03_capd_downloading_images.py')
os.system('../study_2/code/04_mit_election_lab_merge.py')
os.system('../study_2/code/05_color_detection.py')
```

To examine whether the use of colors on yard signs varies systematically depending on the electoral context, using the `selenium` [@selenium] and `time` [@time] `python` [@python] packages, I collect images from 2018, 2020, and 2022 Congressional elections for the House of Representatives across the United States. These yard signs are pulled together on one website by the Center for American Politics and Design[^color_influence_pre-reg_anon-9]. With these data, I detect the percentage of the "Republican Red" and "Democratic Blue" on the yard signs. I then use redistricting data from the [MIT election lab and Redistricting Data Hub](https://redistrictingdatahub.org/data/about-our-data/) to determine districts that did and did not have changes in the 5-year rolling average partisan vote share but that are also nationally representative. As there are a number of external factors that may effect choice in yard sign color besides perceptions of how it may attract voters, this threatens my ability to make claims about any estimation of an Average Treatment Effect [^color_influence_pre-reg_anon-10]. Instead these data are used to perform a synthetic control design to give me causal leverage over the role that electoral context has on the proportion of partisan colors (i.e., red and blue) on yard signs relative to non-partisan colors by estimating a Local Average Treatment Effect [^color_influence_pre-reg_anon-11] on districts that were redistricted but vary in whether the redistricting leads to a partisan gerrymandered district or not. This analysis aims to bear evidence on the hypothesis that campaigns respond to the preferences of partisan voters and adjust their branding as a result. In this case, the branding is the color of the yard sign. First, I'll explain how I collected data on the proportion of partisan and non-partisan colors on these yard signs from the [Center for American Politics and Design](https://www.politicsanddesign.com).

[^color_influence_pre-reg_anon-9]: See: https://www.politicsanddesign.com/

[^color_influence_pre-reg_anon-10]: $\frac{1}{n}\sum{}{i}(Y_i(d=1) - Y_i(d=0))$ [see @lundberg_et-al_2021_asr]

[^color_influence_pre-reg_anon-11]: $E[(Y^1_i - Y^0_i) | d^1_i - d^0_i = 1]$ \[see @cunningham_2021_yup\]

I collected the GOP logo used on their official Twitter account during the 2022 midterm election cycle to provide an example of how color detection works. With the `opencv` package [@opencv_library], I load this image and convert it to a three-dimensional `numpy` [@numpy] array that contains information about the GBR (reversed RBG) values for the pixels in that image. I resized the images to be a standardized 224 $x$ 224 pixels. I train the computer to detect a range of GBR values that encompass the official "Republican Red" [^color_influence_pre-reg_anon-12]. For the broader exercise, I do it for the color white[^color_influence_pre-reg_anon-13] and "Democratic blue" [^color_influence_pre-reg_anon-14]. Once this range of values is specified, the computer detects the pixels that do not contain values within this pre-specified range and converts those values to represent the color black. @fig-color-detection-example presents this process using the `opencv` [@opencv_library] and `matplotlib` [@matplotlib] packages.

[^color_influence_pre-reg_anon-12]: lower values: (93, 9, 12), higher values: (236, 69, 75)

[^color_influence_pre-reg_anon-13]: upper and lower values: (255, 255, 255)

[^color_influence_pre-reg_anon-14]: lower values: (0, 18, 26), higher values: (102, 212, 255)

```{python}
#| label: example detection
#| echo: false

from sys import path # for path management
from cv2 import imread, imwrite # for color detection
    #* user-defined
#path.append("../")
from PY.helper import colorDetector

# Define colors to detect
    #* White
        #** Not defined. Default for colorDetector()
    #* Red
republican_red = [232, 27, 35] # target color
red_lower = [93, 9, 12] # lower end of spectrum for red
red_higher = [237, 69, 75] # higher end of spectrum for red

# Load gop image to read
img = imread("../../assets/img/gop_2022.png")

# Detect colors
percent, img_transformed, result = colorDetector(img = img, color_upper = red_higher, color_lower = red_lower)

transformed= imwrite("../../assets/img/gop_2022_transformed.png", img_transformed) # save transformed image

masked= imwrite("../../assets/img/gop_2022_detected.png", result) # save masked image
```

::: {#fig-color-detection-example layout-ncol="2"}
![Resized original image](../../assets/img/gop_2022_transformed.png){#fig-resized}

![Masked](../../assets/img/gop_2022_detected.png){#fig-masked}

Detecting colors in the GOP logo
:::

I then extract the values in the array that are non-black and calculate the percentage of non-black pixels (as depicted in @eq-img-color-percentage).

$$
\text{Color} \% = \frac{\text{Non-black}}{\text{Transformed}} \times \frac{\text{Original}_{\text{Height}} + \text{Original}_{\text{Width}}}{2\text{Transformed}_{\text{Height}} + 2\text{Transformed}_{\text{Width}}}
$$ {#eq-img-color-percentage}

```{python}
#| output: asis
# Print calculated percentage of example
print("For the example in @fig-color-detection-example, about {:.2f}".format(percent) + " of the image is red.")
```

With these data, I estimate a mixed effects model with intercept random effects at the district level and intercept fixed effects at the election year level to examine the district and within election year differences between choices to employ more "partisan" colors on a yard sign based on historical data on the electoral performance of partisan and non-partisan candidates. That is, I examine whether there are differences between how campaigns respond to redistricting that make the electorate for that constituency more partisan or less partisan while controlling for a number of election-year and campaign specific factors. Support for $H_5$ would suggest that campaigns in districts that are more partisan will shift to use more traditionally-partisan colors while campaigns in districts that do not shift to have a more historically partisan constituency will use fewer of those traditionally-partisan colors.

# Discussion

Not only do yard signs matter for electoral strategy, outcomes, and voter attitudes and perceptions, but I suspect that the yard signs' color partially explains these phenomena.

While yard signs are a simple form of party branding, richer theorizing about how diverse types of potentially politically-relevant information allows us to fill some of these gaps. In the case of yard signs, while some may be skeptical of whether yard signs and campaigns, more generally, matter for electoral outcomes, an application of the snap-judgment model on yard signs suggests that they indeed have potential and provide an explanation as to how it occurs.

Outside of color on yard signs, I suspect that color provides valuable information that is processed efficiently by the public. Due to this, voters can connect colors like Red and Blue to make assumptions about a political candidate's partisan affiliation. With this information, voters can then evaluate the candidate. This is a much simpler task than hearing the candidate's positions on immigration, for example, and then making a connection to their partisan affiliation.

{{< pagebreak >}}